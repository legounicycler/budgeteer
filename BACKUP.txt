%% Optimization 
close all
H = [2 1; 1 2];
b = [-1; -3];
t = true;
i = 0;
%% Initialize state  (x0)
x0 = [0;0]; % remember, this is a 2-D problem, so this vector needs to be 2D

%% Calculate search direction (gradient descent in this case)

%f = .5 * x0' * H * x0 - b'*x0;
syms x y 
f = .5 * [x;y]' * H * [x;y] - b' * [x;y];
grad = gradient(f, [x y]);
figure
fcontour(f)
hold on

%% Update state x(t+1) = x(t) + (step size) * (search direction)

% step size
% (Hint: start with alpha = 1.  What happens if you decrease the step size?
% Does the optimization algorithm give better results?)
alpha = .05;
while (t && i < 1000)
    g = subs(grad, [x,y], x0');
    g1 = -1*double(g)/norm(double(g));
    x0 = x0 + alpha*g1;
    plot(x0(1), x0(2), 'rx')
    i = i+1;
    if (double(norm(g)) < .1)
        t = false;
        x0
        double(subs(f, [x,y], x0'))
    end
end

%% Convergence 
% Think of how to represent convergence in the code.  Basically how do you
% know that you have reached the optimal point? Think of what happens to
% the gradient of the function if we are finding the minimum of a function
% by hand. How does this change for local and global optimal solutions?
